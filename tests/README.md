<img align="right" style="width: 5em;" src="https://github.com/user-attachments/assets/1e97270f-7925-4cc2-8791-8d0cc77fe512">


# Regression Tests for Altinity ClickHouse Helm Charts

Comprehensive regression test suite for Altinity ClickHouse Helm charts, ensuring proper deployment and configuration of ClickHouse clusters in Kubernetes environments using the [TestFlows](https://testflows.com/) framework.

## ğŸ“‹ Table of Contents

* [Overview](#-overview)
  * [Test Architecture](#test-architecture)
  * [Test Flow](#test-flow)
* [Directory Structure](#-directory-structure)
* [Test Coverage](#-test-coverage)
  * [What's Covered](#whats-covered)
  * [Test Scenarios](#test-scenarios)
  * [Coverage Gaps](#coverage-gaps)
* [Supported Environment](#-supported-environment)
* [Prerequisites](#-prerequisites)
  * [Kubernetes Cluster](#kubernetes-cluster)
  * [Helm](#helm)
  * [Python Modules](#python-modules)
* [How to Run Tests](#-how-to-run-tests)
  * [Running All Tests](#running-all-tests)
  * [Running Specific Fixtures](#running-specific-fixtures)
  * [Custom Configuration](#custom-configuration)
* [Contributing](#-contributing)
  * [Adding New Test Fixtures](#adding-new-test-fixtures)
  * [Adding New Test Steps](#adding-new-test-steps)
  * [Best Practices](#best-practices)
* [Troubleshooting](#-troubleshooting)
* [Additional Resources](#-additional-resources)

---

## ğŸ— Overview

### Test Architecture

The test suite follows a **layered architecture** built on TestFlows:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          run/smoke.py (Entry Point)             â”‚
â”‚  Defines test configuration and runs features   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      scenarios/smoke.py (Test Scenarios)        â”‚
â”‚  Orchestrates test flows and fixture execution  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    steps/ (Reusable Test Operations)            â”‚
â”‚  â€¢ clickhouse.py - ClickHouse operations        â”‚
â”‚  â€¢ kubernetes.py - K8s resource management      â”‚
â”‚  â€¢ helm.py - Helm chart operations              â”‚
â”‚  â€¢ users.py - User & permission verification    â”‚
â”‚  â€¢ deployment.py - HelmState orchestrator       â”‚
â”‚  â€¢ minikube.py - Local cluster management       â”‚
â”‚  â€¢ system.py - System utilities                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         fixtures/ (Test Data)                   â”‚
â”‚  YAML configurations for different deployments  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Test Flow

Each test scenario follows this sequence:

1. **Setup Phase**
   - Configure minikube environment (optional)
   - Set kubectl context
   - Build Helm dependencies

2. **Deployment Phase**
   - Install/upgrade ClickHouse using Helm with fixture values
   - Wait for pods to be created and running

3. **Verification Phase**
   - **HelmState Orchestrator** (`deployment.py`) reads fixture and determines checks
   - Delegates verification to specialized step functions:
     - Pod count and status
     - Cluster topology (shards/replicas)
     - Replication health and functionality
     - User authentication and permissions
     - Persistence (PVCs, storage sizes)
     - Service endpoints and load balancers
     - Pod/service annotations and labels
     - Custom ClickHouse configuration (extraConfig)
     - Keeper availability and storage
     - Resource limits and requests
     - Metrics endpoint accessibility

4. **Chaos Testing** (for replicated setups)
   - Keeper high availability tests
   - Pod failure recovery

5. **Cleanup Phase**
   - Uninstall Helm release
   - Delete namespace
   - Clean up test resources

---

## ğŸ“ Directory Structure

```
tests/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”‚
â”œâ”€â”€ run/                         # Test entry points
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ smoke.py                 # Main test runner (starts here!)
â”‚
â”œâ”€â”€ scenarios/                   # Test orchestration
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ smoke.py                 # Smoke test scenarios (fixtures + upgrades)
â”‚
â”œâ”€â”€ steps/                       # Reusable test operations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ clickhouse.py            # ClickHouse-specific operations (1383 lines)
â”‚   â”œâ”€â”€ kubernetes.py            # Kubernetes resource management
â”‚   â”œâ”€â”€ helm.py                  # Helm install/upgrade/uninstall
â”‚   â”œâ”€â”€ deployment.py            # HelmState orchestrator
â”‚   â”œâ”€â”€ users.py                 # User verification and permissions
â”‚   â”œâ”€â”€ minikube.py              # Local cluster setup
â”‚   â””â”€â”€ system.py                # Shell command utilities
â”‚
â”œâ”€â”€ fixtures/                    # Test configurations (YAML)
â”‚   â”œâ”€â”€ 01-minimal-single-node.yaml          # Baseline: 1 node
â”‚   â”œâ”€â”€ 02-replicated-with-users.yaml        # 3 replicas + 3 keepers + users
â”‚   â”œâ”€â”€ 03-sharded-advanced.yaml             # 3 shards Ã— 2 replicas + 5 keepers
â”‚   â”œâ”€â”€ 04-external-keeper.yaml              # External keeper (commented out)
â”‚   â”œâ”€â”€ 05-persistence-disabled.yaml         # Ephemeral storage
â”‚   â”œâ”€â”€ 06-eks-multi-zone-production.yaml    # Production-like EKS config
â”‚   â”œâ”€â”€ 07-eks-io-optimized.yaml             # I/O optimized EKS config
â”‚   â””â”€â”€ upgrade/
â”‚       â”œâ”€â”€ initial.yaml                     # Pre-upgrade state
â”‚       â””â”€â”€ upgrade.yaml                     # Post-upgrade state
â”‚
â”œâ”€â”€ helpers/                     # Test utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ argparser.py             # Command-line argument parser
â”‚
â””â”€â”€ requirements/                # Requirement definitions
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ helm.py                  # Helm requirement checks
    â””â”€â”€ helm.md                  # Helm requirement documentation
```

---

## âœ… Test Coverage

### What's Covered

The test suite provides comprehensive coverage across multiple dimensions:

#### **1. Deployment Configurations**
- âœ… Single-node minimal deployment
- âœ… Multi-replica deployments (2-3 replicas)
- âœ… Sharded clusters (up to 3 shards Ã— 2 replicas)
- âœ… Keeper integration (3-5 replicas)
- âœ… External keeper configurations
- âœ… Persistence (enabled/disabled)
- âœ… Ephemeral storage deployments

#### **2. ClickHouse Functionality**
- âœ… Version verification
- âœ… Connection testing
- âœ… Query execution
- âœ… Cluster topology (system.clusters)
- âœ… Replication health (system.replicas)
- âœ… Data replication verification (create + replicate test tables)
- âœ… Metrics endpoint accessibility
- âœ… Custom configuration (extraConfig XML)
- âœ… Keeper high availability (chaos tests)

#### **3. User Management**
- âœ… Default user authentication
- âœ… Multiple user creation
- âœ… SHA256 password hashing
- âœ… User grants and permissions
- âœ… Access management settings
- âœ… Host IP restrictions (network policies)
- âœ… Read-only user verification

#### **4. Kubernetes Resources**
- âœ… Pod creation and readiness
- âœ… Service endpoints
- âœ… Persistent Volume Claims (PVCs)
- âœ… Storage size verification
- âœ… Access modes (ReadWriteOnce, etc.)
- âœ… Pod annotations and labels
- âœ… Service annotations and labels
- âœ… LoadBalancer services with source ranges
- âœ… Service accounts
- âœ… Secrets management

#### **5. Advanced Features**
- âœ… Anti-affinity configurations
- âœ… Node selectors
- âœ… Tolerations
- âœ… Topology spread constraints
- âœ… Resource limits and requests (CPU, memory)
- âœ… Log persistence (separate volumes)
- âœ… Custom namespace domain patterns
- âœ… Cluster secrets
- âœ… Custom names (nameOverride)

#### **6. Upgrade Scenarios**
- âœ… Helm chart upgrades
- âœ… Configuration changes
- âœ… Data survival verification
- âœ… In-place vs. cluster replacement upgrades
- âœ… Topology changes during upgrade

### Test Scenarios

Currently implemented test scenarios:

| Scenario | Fixture | Pods | Description |
|----------|---------|------|-------------|
| **Minimal Deployment** | `01-minimal-single-node.yaml` | 1 | Baseline test: single ClickHouse node, no keeper |
| **Replicated + Users** | `02-replicated-with-users.yaml` | 6 | 3 replicas + 3 keepers, comprehensive user setup |
| **Sharded Advanced** | `03-sharded-advanced.yaml` | 11 | 3 shards Ã— 2 replicas + 5 keepers, advanced K8s features |
| **External Keeper** | `04-external-keeper.yaml` | 4 | Uses external keeper (currently disabled in tests) |
| **Ephemeral Storage** | `05-persistence-disabled.yaml` | 5 | No persistent volumes, 2 replicas + 3 keepers |
| **EKS Multi-Zone** | `06-eks-multi-zone-production.yaml` | TBD | Production-like EKS configuration |
| **EKS I/O Optimized** | `07-eks-io-optimized.yaml` | TBD | I/O optimized EKS configuration |
| **Upgrade Test** | `upgrade/initial.yaml` â†’ `upgrade/upgrade.yaml` | Variable | Tests upgrade path and data survival |

**Currently Active Tests**: Fixtures 01, 02, and upgrade scenario  
**Commented Out**: Fixtures 03, 04, 05 (TODO)

### Coverage Gaps

Areas that **need additional testing or are not fully covered**:

#### **1. Missing Test Coverage**
- âŒ **Backup and restore** - No automated backup/restore testing
- âŒ **Disaster recovery** - No full cluster failure scenarios
- âŒ **Network policies** - Limited testing of K8s network restrictions
- âŒ **Monitoring integration** - Prometheus scraping tested only via annotations
- âŒ **Logging integration** - No FluentD/ElasticSearch integration tests
- âŒ **Multi-cluster** - No federation or distributed query tests
- âŒ **Horizontal scaling** - No dynamic scale-up/scale-down tests
- âŒ **Performance benchmarks** - No load testing or performance metrics
- âŒ **Cloud-specific features** - AWS EKS, GKE, AKS specific integrations
- âŒ **Storage classes** - Limited testing of different storage backends
- âŒ **Resource quotas** - No namespace quota testing
- âŒ **RBAC** - Limited Kubernetes RBAC testing
- âŒ **Init containers** - No custom init container testing
- âŒ **Sidecar containers** - No sidecar pattern testing

#### **2. Partial Coverage**
- âš ï¸ **Metrics** - Only endpoint accessibility tested, not actual metric values
- âš ï¸ **Keeper HA** - Basic chaos test exists, but limited scenarios
- âš ï¸ **Upgrade paths** - Only one upgrade scenario tested
- âš ï¸ **Configuration drift** - No testing of manual changes vs. Helm state
- âš ï¸ **Resource exhaustion** - No OOM or disk full scenarios
- âš ï¸ **Long-running stability** - Tests are short-lived (minutes, not hours/days)
- âš ï¸ **TLS/SSL** - Only tests configuration and setup, does not test actual encryption


---

## ğŸŒ Supported Environment

- **Operating System**: [Ubuntu](https://ubuntu.com/) 22.04 / 24.04
- **Python**: >= 3.10.12, <= 3.12 (3.13+ has `lzma` package that is incompatible with the test framework)
- **Kubernetes**: >= 1.24
- **Helm**: >= 3.8.0
- **Minikube**: >= 1.28.0 (for local testing)
- **Docker**: Required as Minikube driver
  - (alternatively) **OrbStack**: >= 2.0
- **kubectl**: Latest stable version

---

## ğŸ“¦ Prerequisites

### Kubernetes Cluster

You need access to a Kubernetes cluster. For **local testing**, two providers are supported:

#### Option 1: Minikube (default)

```bash
# Install Minikube
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube

# Verify installation
minikube version
```

#### Option 2: OrbStack (account required, may need license)

1. Install OrbStack by following the [_Quick start_ guide \(docs.orbstack.dev\)](
   https://docs.orbstack.dev/quick-start#installation).
2. Enable Kubernetes in OrbStack:
    1. Open the OrbStack app
    2. Go to Settings... (`Cmd âŒ˜ + ,`) â†’ Kubernetes
    3. Toggle the `Enable Kubernetes cluster` option
    4. Click the `Apply and Restart` button
3. Verify that OrbStack is running.
    ```sh
    $ orb status
    # Running
    ```
4. Verify the Kubernetes context.
    ```sh
    $ kubectl config get-contexts orbstack
    # CURRENT   NAME       CLUSTER    AUTHINFO   NAMESPACE
    # *         orbstack   orbstack   orbstack
    ```

### Helm

Install Helm 3:

```bash
# Install Helm
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# Verify installation
helm version
```

### Python Modules

Install required Python dependencies:

```bash
# From the repository root
pip3 install -r tests/requirements.txt
```

**Dependencies**:
- `testflows==2.4.13` - Test framework
- `testflows.texts==2.0.211217.1011222` - Text utilities
- `PyYAML==6.0.1` - YAML parsing
- `requests==2.32.3` - HTTP requests

---

## ğŸš€ How to Run Tests

### Running All Tests

To run the complete test suite (all active fixtures + upgrades):

```bash
# From the repository root
# With Minikube (default)
python3 ./tests/run/smoke.py

# With OrbStack
LOCAL_K8S_PROVIDER=orbstack python3 ./tests/run/smoke.py
```

This will:
1. \[Minikube only\] Start/restart Minikube with 4 CPUs and 6GB memory
2. Run all enabled fixture deployments
3. Run upgrade scenarios
4. \[Minikube only\] Clean up and delete Minikube

**Expected Duration**: 10 minutes 

### Running Specific Fixtures

To run tests for a specific configuration, modify `tests/scenarios/smoke.py`:

```python
# Edit FIXTURES list to include only desired fixtures
FIXTURES = [
    "fixtures/01-minimal-single-node.yaml",  # Only run minimal test
]
```

Or comment out the upgrade tests:

```python
@TestFeature
@Name("comprehensive")
def feature(self):
    """Run all comprehensive smoke tests."""
    
    with Given("minikube environment"):
        minikube.setup_minikube_environment()
        kubernetes.use_context(context_name="minikube")
    
    Feature(run=check_all_fixtures)
    
    # Feature(run=check_all_upgrades)  # Commented out
```

### Custom Configuration

You can override test settings by modifying `tests/run/smoke.py`:

```python
@TestModule
@Name("smoke")
@ArgumentParser(argparser)
def regression(self, feature):
    """Execute smoke tests."""
    
    # Use remote chart instead of local
    self.context.altinity_repo = "https://helm.altinity.com"
    self.context.version = "25.3.6.10034.altinitystable"
    
    # Or use local chart (default)
    self.context.local_chart_path = os.path.join(os.getcwd(), "charts", "clickhouse")
    
    Feature(run=load(f"tests.scenarios.smoke", "feature"))
```

**Minikube resource customization** in `tests/steps/minikube.py`:

```python
def setup_minikube_environment(self, cpus=4, memory="6g", clean_up=True):
    # Increase resources for larger deployments
    # cpus=8, memory="12g"
```

---

## ğŸ¤ Contributing

### Adding New Test Fixtures

Test fixtures define Helm values for different deployment scenarios.

**1. Create a new fixture file**:

```bash
# Create a new fixture
touch tests/fixtures/08-my-custom-config.yaml
```

**2. Define the configuration**:

```yaml
---
# tests/fixtures/08-my-custom-config.yaml
# Custom configuration for testing feature X
# Expected pods: 2 ClickHouse + 3 Keeper = 5 total
nameOverride: "custom"

clickhouse:
  replicasCount: 2
  shardsCount: 1
  
  defaultUser:
    password: "CustomPassword123"
  
  persistence:
    enabled: true
    size: 5Gi
  
  # Add your custom configuration here
  service:
    type: LoadBalancer

keeper:
  enabled: true
  replicaCount: 3

operator:
  enabled: true
```

**3. Add fixture to test scenarios**:

```python
# Edit tests/scenarios/smoke.py
FIXTURES = [
    "fixtures/01-minimal-single-node.yaml",
    "fixtures/02-replicated-with-users.yaml",
    "fixtures/08-my-custom-config.yaml",  # Add your fixture
]
```
**4. Add missing verification steps in HelmState and step files if needed**.

**5. Run the test**:

```bash
python3 ./tests/run/smoke.py
```

### Adding New Test Steps

Test steps are reusable operations in the `tests/steps/` directory.

**Example: Adding a new ClickHouse verification step**:

```python
# In tests/steps/clickhouse.py

@TestStep(Then)
def verify_custom_setting(self, namespace, expected_value, admin_password=""):
    """Verify a custom ClickHouse setting."""
    clickhouse_pods = get_clickhouse_pods(namespace=namespace)
    assert len(clickhouse_pods) > 0, "No ClickHouse pods found"
    
    pod_name = clickhouse_pods[0]
    
    query = "SELECT value FROM system.settings WHERE name = 'custom_setting'"
    result = execute_clickhouse_query(
        namespace=namespace,
        pod_name=pod_name,
        query=query,
        user="default",
        password=admin_password,
        check=True
    )
    
    actual_value = result.stdout.strip()
    assert actual_value == expected_value, \
        f"Expected custom_setting={expected_value}, got {actual_value}"
    
    note(f"âœ“ Custom setting verified: {expected_value}")
```

**Using the new step in HelmState** (`tests/steps/deployment.py`):

```python
class HelmState:
    # ... existing code ...
    
    def verify_custom_config(self, namespace):
        """Verify custom configuration."""
        custom_value = self.clickhouse_config.get("customSetting")
        
        if custom_value:
            admin_password = self.clickhouse_config.get("defaultUser", {}).get("password", "")
            clickhouse.verify_custom_setting(
                namespace=namespace,
                expected_value=custom_value,
                admin_password=admin_password
            )
            note(f"âœ“ Custom config verified")
    
    def verify_all(self, namespace):
        # ... existing checks ...
        
        if self.clickhouse_config.get("customSetting"):
            self.verify_custom_config(namespace=namespace)
```

### Best Practices

When contributing tests, follow these guidelines:

#### **1. Test Independence**
- âœ… Tests should be **idempotent** (can run multiple times)
- âœ… Always **clean up** resources in `Finally` blocks
- âœ… Don't rely on state from previous tests
- âœ… Use unique namespaces per test

#### **2. Error Handling**
- âœ… Add proper **assertions** with descriptive messages
- âœ… Use **timeouts** to prevent hanging tests
- âœ… Log detailed **debugging info** on failures
- âœ… Handle edge cases (e.g., missing pods, failed queries)

#### **3. Code Organization**
- âœ… Keep **steps** reusable and atomic
- âœ… Put orchestration logic in **scenarios**
- âœ… Store configuration in **fixtures**
- âœ… Use descriptive function and variable names

#### **4. Documentation**
- âœ… Add **docstrings** to all functions
- âœ… Include **comments** explaining complex logic
- âœ… Document **expected behavior** in fixtures
- âœ… Update this README when adding major features

#### **5. TestFlows Conventions**
- âœ… Use `@TestStep` decorators with appropriate levels:
  - `@TestStep(Given)` - Setup operations
  - `@TestStep(When)` - Actions
  - `@TestStep(Then)` - Assertions
  - `@TestStep(Finally)` - Cleanup
- âœ… Use `note()` for informational messages
- âœ… Use `with Given/When/Then/And/Finally` blocks for clarity
- âœ… Leverage context variables (`self.context`) for shared state

---

## ğŸ”§ Troubleshooting


### Manual Testing

To manually replicate a test:

```bash
# Start Minikube
minikube start --driver=docker --cpus=4 --memory=6g

# Set context
kubectl config use-context minikube

# Install chart with fixture
helm install test-deploy ./charts/clickhouse \
  --namespace test-ns \
  --create-namespace \
  --values tests/fixtures/01-minimal-single-node.yaml

# Wait and verify
kubectl get pods -n test-ns -w

# Cleanup
helm uninstall test-deploy -n test-ns
kubectl delete namespace test-ns
```



---

## ğŸ“š Additional Resources

- **TestFlows Documentation**: https://testflows.com/
- **Helm Charts Repository**: https://github.com/Altinity/clickhouse-operator
- **ClickHouse Documentation**: https://clickhouse.com/docs/
- **Kubernetes Documentation**: https://kubernetes.io/docs/

---

## ğŸ“ License

This test suite is part of the Altinity ClickHouse Helm Charts repository and follows the same license.

---

**Happy Testing! ğŸš€**

For questions or issues, please open an issue in the GitHub repository.
